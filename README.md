# The Task Space: An Integrative Framework for Team Research

*Management Science*, Accepted October 2025

Author(s): Xinlan Emily Hu, Mark E. Whiting, Linnea Gandhi, Duncan J. Watts, and Abdullah Almaatouq

## 1.	Overview

This repository contains the data and code required to reproduce the results in `The Task Space: An Integrative Framework for Team Research`. This paper presents a novel multidimensional representation of tasks, in which the task performed by a group can be described along 24 quantifiable dimensions that are grounded in prior theory. It also presents a case study in which the 24-dimensional Task Space is used to select a set of 20 diverse tasks, and subsequently to predict heterogeneity in *group advantage* (the ability of groups to outperform an equivalent individual) across the 20 tasks.

The data and code in this paper are divided into two distinct components:

(1) **The Task Map**: A database of 102 tasks sourced from the interdisciplinary literature on group collaboration, annotated by human raters (sourced from Amazon Mechanical Turk) along the 24 dimensions. Here, the relevant empirical data is from the annotation process, and the analysis code transforms the raw ratings into final task dimensions.

(2) **Group Advantage Integrative Experiment**: A large-scale experiment in which 1,200+ workers on Amazon Mechanical Turk are recruited to complete tasks in real time. We then use the 24 task dimensions to predict the extent to which groups outperform equivalent individuals (*group advantage*). Here, the relevant empirical data comes from this experiment, and the analysis explores the question, *when (on which tasks) do groups outperform individuals?*

For more information about this work, please visit our companion website, https://taskmap.seas.upenn.edu/

### Repository Structure:

#### 1. `data/` subfolder
This folder contains all *raw* input data associated with the paper (from both the Task Map annotation and the Group Advantage Integrative Experiment).
A data dictionary (`data_dictionary.md`) details all variables from the datasets in this folder. 

Data from Task Map annotation are at the top level of the folder; data from the Group Advantage experiment, which took place in three waves, are available in subfolders `Wave 1 data/`, `Wave 2 data/`, and `Wave 3 data/`, respectively.

- Information about curating the panel (e.g., rater training and pre-testing) can be found under `task space resources/rating pipelines/`, with the rater panel (with real MTurk ID's hashed) and their pre-test scores included in `rating pipelines/survey_workflows/our_panel.csv`.

- Information about the participants in the Group Advantage study (with real MTurk ID's hashed) can be found in `data/players/`.

#### 2. `analysis/` subfolder
This folder contains all analysis scripts required to reproduce key figures, tables, and findings.

- Analysis for the Task Map can be found under `analysis_task_space/`.

- Analysis for the Group Advantage study can be found under `analysis_group_advantage/`.

- A master script, `run_master.py`, consolidates all key analysis in a single file and logs outputs in `outputs/logs/`.

#### 3. `outputs/`subfolder
This folder contains all outputs that are generated by the code in `analysis/`. Figures, tables, and results are reproduced here.

- `processed_data/` are data files generated from the raw inputs. This includes the **Task Map** (`ouptuts/processed_data/task_map.csv`), which is the data object associated with this paper's primary contribution.

- `figures/` are images and visuals generated by the code.

- `cached_pkls/` are `.pkl` files that cache objects that may take slightly longer to reproduce, such as models.

- `logs/` are printed outputs from running the master script.

#### 4. `task space resources/` subfolder

This folder contains the primary assets for using and reproducing the crowd-annotated Task Space. It contains two subfolders: `writeups/` (in which each task included in the repository is a separate Markdown file) and `rating pipelines/` (which documents the process for training crowd annotators).

The top level of this folder contains cleaned up `.csv` files of the the 24 task dimensions, 102 tasks, and the questions used for annotation.

#### 5. `multi-task-empirica/` subfolder

This folder contains the assets and code required for running the large-scale group experiments on Empirica. This folder is a Git submodule; it links to another GitHub repository, which manages the version control for its contents.

<!-- INSTRUCTIONS: The typical README serves the purpose of guiding a reader through the available material and a route to replicating the results in the research paper. Start by providing a brief overview of the available material and a brief guide as to how to proceed from beginning to end. -->

## 2.	Data availability and provenance

Data provenance: **This paper relies on data, and all data necessary to reproduce the results of the paper are included. Below is a detailed description of how the data was obtained, allowing future researchers to reproduce the dataset.**

All data originates from the authors' own data collection and is provided freely in the `data/` directory for other researchers' usage in academic applications.

Data collection took place online, via Amazon Mechanical Turk. Task Space ratings were collected between July 2022 and March 2023. The Group Advantage experimental data was colleced between April 2023 and April 2024.

For detailed descriptions of the data collection process, please refer to our manuscript.

### Structure and description of the `data/` directory's contents

- `players/` — Contains information about participants in the Group Advantage experiment, including individual-level data (individuals.csv) and player data separated by experimental wave and epoch (e.g., players_wave1_epoch1.csv).

- `Wave 1 data/`, `Wave 2 data/`, and `Wave 3 data/` — Contain the primary experimental data for each of the three data collection waves of the Group Advantage experiment. Each wave includes one or more epochs (e.g., Epoch 1, Epoch 2) that share a common structure of CSV and JSON tables (e.g., games, players, rounds, stages, treatments, etc.). The contents and variables of these files are described in detail in `data_dictionary.md`.

- `raw_map.csv` — Contains the raw crowd annotation data used to characterize tasks along the 24 task-dimension ratings for the construction of the Task Space.

- `hand_labeled_task_mcgrath_sectors.csv` — Contains the hand-labeled mappings of tasks into canonical task categories following McGrath’s task typology. Used for visualization/comparison purposes.

### Data processing overview

The steps to process the Task Space rating data are detailed in `analysis/analysis_task_space/generate_task_map_from_raw.R`. Following these steps produces the main Task Map data object.

The steps to process the Group Advantage experimental data are detailed in `analysis/analysis_group_advantage/raw_data_cleaning.R`. Following these steps produces the data table used for answering the primary research question for the case study: *when (on which tasks) do groups outperform individuals?*

### Data collection materials

Materials associated with data collection are provided in this repository and located in the following subfolders:

#### `task space resources/` subfolder

This folder contains the materials documenting the process of creating the Task Space. Clean versions of the 102 task descriptions, 24 dimensions, and rater questions are provided at the top level.

The `rating pipelines` subfolder documents the process of recruiting and training humans to annotate the Task Map (our repository of 102 tasks, annotated along 24 quantitative dimensions by raters recruited from Amazon Mechanical Turk). 

The following materials may be useful to future researchers wishing to reproduce our rating pipeline:

- `rating pipelines/rater_training/` contains the Qualtrics survey used to instruct raters on how to annotate the tasks.
- `rating pipelines/resources_for_rating/` contains data used in the task rating process, including the questions and exact descriptions provided to the raters.
- `rating pipelines/survey_workflows/` contains code associated with grading the pre-test for filterings raters, as well as the panel of rater comprising our final pool.
- `rating pipelines/writeup to html pipeline/` contains code and materials associated with translating the written task summaries (which are stored in `.md` format in the `writeups/` folder) into html files, which was the format in which they were displayed to raters.

#### `multi-task-empirica/` subfolder

This folder contains the materials required to conduct real-time group experiments on the Empirica platform. Documentation for the Empirica code is provided within the folder; individual task implementations (which include all stimuli and participant instructions) can be found in `multi-task-empirica/multi-task-app/customTasks/`. Folders in that directory correspond to the code for implementing each of the 20 tasks in the experiment.

<!-- INSTRUCTIONS: The README should contain a description of the origin (provenance), location and accessibility of the data used in the article. 

When secondary data are used (whether the data are included in the package, or not), the description of the provenance should describe the condition under which (a) the current authors (b) any future users might access the data.  Provide details on how datasets can be obtained (including contact details of provider, date ranges, versions, exclusions, etc.), how they need to be merged if applicable, and where they need to be put under which names for the code to run. The idea is that after a researcher follows your step-by-step instructions, they can readily run your code and (re)produce your results.

When the data were generated by the authors, e.g. collected from the web, in online, lab or field experiments, through surveys, then the description of the provenance should describe the data generating process, e.g., the survey or experimental procedures.

The information should describe ALL data sets and data sources used, regardless of whether they are provided as part of the replication package or not, and regardless of size or scope. For each dataset, list the file and describe the source. If there are many datasets, a list in tabular form can be useful. If some or all datasets are provided as sample or synthetic or mockup data, please indicate prominently, since they typically will not reproduce your results. Describe the generating process (provide code if applicable). -->


## 3.	Variable dictionaries

A data dictionary for all datasets can be found in `data/data_dictionary.md`.

<!-- INSTRUCTIONS: Include data dictionaries for all used datasets (whether included or not). Each data dictionary lists all variables with names as used in the code/dataset, with a one-line description of the variable. -->


## 4.	Computational requirements

Both R and Python are required to reproduce this package. The code has been reproduced with R version 4.5.1 and Python version 3.13.7.

Nonstandard package requirements are listed separately for the two components:

- Requirements for the Task Map can be found under `analysis_task_space/requirements.txt`.

- Requirements for the Group Advantage study can be found under `analysis_group_advantage/requirements.txt`.

<!-- INSTRUCTIONS: List any software used to run your code, and their versions. List any non-standard packages (e.g. Stata or R packages, python libraries) and their versions, if applicable. If using non-standard software, provide instructions on how to obtain it. If the code includes scripts that install packages, document here as well. If your code uses random numbers, use a fixed seed such that your results can be reproduced.

If the runtime of the code is larger than a few minutes on a regular computer, indicate approximate runtimes. If relevant, describe necessary hardware requirements. -->

## 5.	Programs/Code

All programs and code associated with the project can be found in the `analysis` directory.

Main analyses associated with each of the two components can be reproduced by running commands via the master script (`analysis/run_master.py`).

```
  # list available steps
  python analysis/run_master.py --list

  # run only the task space pipeline
  python analysis/run_master.py --steps task_space

  # run group advantage pipeline end-to-end
  python analysis/run_master.py --steps group_advantage

  # run both
  python analysis/run_master.py --steps task_space,group_advantage
```

### Location of main empirical figures and tables

- **Figure 4 (Heterogeneity in group advantage across conditions)**: `raw_data_cleaning.R`
- **Figure 5 (Bar plots of the root mean squared errors of models predicting condition-level group advantage)**: `viz.ipynb` (underlying models generated by `models.ipynb`)
- **Figure 6 (Feature importance among task dimensions)**: `viz.ipynb`
- **Table 1 (A summary of the 24 dimensions included in the Task Space)** is included as `task space resources/24_dimensions_clean.csv`.

### Location of key figures in appendices
- **Appendix Figure 4 (A heatmap in which each row represents a task, and each column represents a task dimension)**: `viz.ipynb`
- **Appendix Figure 5 (A two-dimensional projection of the Task Space using PCA)**: `clean_task_space_visuals.R`
- **Demographic distribution of participants (Appendix Figures 7, 9-11)**: `demographics_viz.ipynb`
- **Appendix Figure 46 (Empirical difference distributions in model performance for the primary ElasticNet models)**: `viz.ipynb`
- **Appendix Figure 47 (Robustness of model performance to adding noise to training data columns)**: `viz.ipynb`
- **Appendix Figure 48 (Results of the second robustness check)**: `viz.ipynb`

Remaining figures in the main manuscript are illustrations and are not generated via empirical data.

Additional analysis (associated with the Supplementary Information) can be found in the relevant subfolders under the `analysis` subdirectory.

<!-- INSTRUCTIONS: Provide detailed instructions on how to run your code. In particular, describe in which order which code files need to be run in order to reproduce all figures and tables in your paper. Indicate where in the code which table/figure/result is produced.

Please include the code as executable files/scripts (not as PDFs). Make sure to include code to reproduce all figures, tables, and other results in the main manuscript. Code for results in the appendix is highly appreciated but not compulsory. Make sure your code is well-documented and uses only relative pathnames (it should run on any computer, not just yours). If there are many individual code files, provide a Master script if possible. -->