---
title: "test-4-task-irr"
author: "Emily Hu"
date: "4/6/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# use an adjusted agreement - https://stats.stackexchange.com/questions/409279/krippendorffs-alpha-and-other-irr-stats-giving-counter-intuitive-results
# install.packages("devtools")
#devtools::install_github("jmgirard/agreement")

if (!require("pacman")) install.packages("pacman")
pacman::p_load(agreement,stats,corrplot,irr,stringr,markdown,ggplot2,dplyr,tidyverse,qualtRics,DescTools, plotly)
```

# Pull in Survey Data
```{r}
surveys <- all_surveys()

task_htmls <- read_csv('./html/task-htmls.csv')
all_tasks <- task_htmls$Task.Name

df.task_name_to_index <- cbind(all_tasks, rep(1:length(all_tasks))) %>%
                                as.data.frame() %>%
                                rename(
                                  task_name = all_tasks,
                                  task_num = V2
                                ) %>%
                                mutate(
                                  task_num = strtoi(task_num)
                                )

#task_mapping_irr_survey_id <- surveys$id[2] # 2 is the RA version of the survey
task_mapping_irr_survey_id <- surveys$id[5] # 5 is the MTurk version of the survey
```

# Define the task map and the main questions
```{r}
df.task_map <- fetch_survey(
  surveyID = task_mapping_irr_survey_id,
  save_dir = ".",
  force_request = TRUE
)

# Question names
main_questions <- c(
  "Q1concept_behav",
  "Q2intel_manip_1",
  "Q3type_1_planning",
  "Q4type_2_generate",
  "Q5creativity_input_1",
  "Q6type_5_cc",
  "Q7type_7_battle",
  "Q8type_8_performance",
  "Q9divisible_unitary",
  "Q10maximizing",
  "Q11optimizing",
  "Q13outcome_multip",
  "Q14sol_scheme_mul",
  "Q15dec_verifiability",
  "Q16shared_knowledge",
  "Q17within_sys_sol",
  "Q18ans_recog",
  "Q19time_solvability",
  "Q20type_3_type_4",
  "Q21intellective_judg_1",
  "Q22confl_tradeoffs",
  "Q23ss_out_uncert",
  "Q24eureka_question"
)

feedback_questions <- c(
  "Q1concept_beh_feedbk",
  "Q2intel_manip_fdbk",
  "Q3type_1_feedback",
  "Q4type_2_feedbck",
  "Q5creativity_feedbck",
  "Q6type_5_feedback",
  "Q7type_7_feedback",
  "Q8type_8_feedback",
  "Q9div_unitary_feedbk",
  "Q10maximizing_feedbk",
  "Q11optimizing_feedbk",
  "Q13out_mult_feedbk",
  "Q14sol_s_mult_fdbk",
  "Q15d_verif_feedbk",
  "Q16shared_kno_feedbk",
  "Q17within_sys_feedbk",
  "Q18ans_recog_feedbk",
  "Q19time_solv_feedbk",
  "Q20type_3_4_feedbk",
  "Q21int_jud_feedbk",
  "Q22con_trade_feedbk",
  "Q23ss_outc_u_feedbk",
  "Q24eureka_q_fdbk"
)

confidence_questions <- c(
  "Q1concept_beh_conf",
  "Q2intel_manip_conf",
  "Q3type_1_conf",
  "Q4type_2_conf",
  "Q5creativity_conf",
  "Q6_type_5_conf",
  "Q7type_7_conf",
  "Q8_type_8_conf",
  "Q9div_unitary_conf",
  "Q10maximizing_conf",
  "Q11optimizing_conf",
  "Q13out_mult_conf",
  "Q14sol_s_mult_conf",
  "Q15d_verif_conf",
  "Q16shared_kno_conf",
  "Q17within_sys_conf",
  "Q18ans_recog_conf",
  "Q19time_solv_conf",
  "Q20type_3_4_conf",
  "Q21int_jud_conf",
  "Q22con_trade_conf",
  "Q23ss_out_u_conf",
  "Q24eureka_q_conf"
)

df.task_map <- df.task_map %>% # need to only filter for complete responses
  filter(Finished == T)
```


## Filtering out some irrelevant data

Handle the case of repeated measures --- take the last measure in case someone does it twice.
```{r}
df.task_map <- df.task_map %>%
  group_by(name) %>%
  slice(n()) %>% ungroup() %>%
  filter()
```

Filtering out all updates after Friday - when changes were made
```{r}
df.task_map <- df.task_map %>% filter(
  EndDate > "2022-04-07 17:58:18	"
)
```

# Set up Summary Statistics

```{r}
df.all_ratings_raw <- df.task_map %>%
  select(c(matches('[0-9]_'))) %>%
  mutate(across(everything(), as.character)) %>%
  cbind(df.task_map %>% 
          mutate(name = as.factor(name))%>% select(name)) %>%
  group_by(name) %>% # Group by the rater
  pivot_longer(
    cols = -c(name),
    names_sep = 2,
    names_to = c("task_num", "question_name")
  ) %>%
  mutate(task_num = strtoi(gsub("[^0-9]", "", task_num)),
         question_name = str_extract(question_name, "Q[0-9].*")) %>% 
  drop_na() %>%
  group_by(task_num) %>%
  inner_join(df.task_name_to_index,
            by = "task_num")

# main summary stats
df.main_questions_summary <- df.all_ratings_raw %>%
  filter(question_name %in% main_questions) %>%
  filter(question_name != 'Q2intel_manip_1' & 
            question_name != 'Q21intellective_judg_1' &
            question_name != 'Q5creativity_input_1') %>%
  mutate(value = recode(value, 
                     "Mental" = 0,
                     "Physical" = 1,
                     "Not applicable or not answerable based on the task description (Please Elaborate Below.)" = 3,
                     "No" = 0,
                     "Yes" = 1
                   )) %>%
  filter(value != 3) %>% # remove cases in which people thought it wasn't answerable - for now
  group_by(task_name,question_name) %>%
  summarize(mean_rating = mean(value),
            n_labels = agree(t(value))$raters,
            all_values = paste(value, collapse = " & "),
            agreement = ifelse(mean_rating > 0.5, mean_rating, 1-mean_rating),
            general.alpha = (agreement-0.5)/(0.5)
            )

df.task_map_summary <- df.main_questions_summary %>%
  filter(n_labels > 1) %>%
  group_by(task_name) %>% # can do task_name, question_name
  summarize(
    n_labels = round(mean(n_labels),0),
    mean_agreement = mean(agreement),
    mean_alpha = mean(general.alpha)
  )

df.task_map_summary %>% arrange(desc(mean_alpha))
```

Take a look at a summary by task.
```{r}
task_based_summary <- df.main_questions_summary %>%
  filter(n_labels > 1) %>%
  group_by(question_name) %>% # can do task_name, question_name
  summarize(
    n_labels = round(mean(n_labels),0),
    mean_agreement = mean(agreement),
    mean_alpha = mean(general.alpha)
  ) %>%
  arrange(desc(mean_alpha))

task_based_summary
```

Export the data in a way that is useful to James -- 4/15/2022
```{r warning=FALSE}
df.james.bayesian <- df.all_ratings_raw %>%
  mutate(value = recode(value,
                     "Mental" = 0,
                     "Physical" = 1,
                     "Not applicable or not answerable based on the task description (Please Elaborate Below.)" = 3,
                     "No" = 0,
                     "Yes" = 1
                   )) %>%
  filter(
    question_name %in% main_questions
  ) %>% ungroup() %>%
  select(task_name, question_name, name, value) %>% drop_na() 


df.james.bayesian <- df.james.bayesian %>%
  left_join(df.task_map %>% select(name, `Duration (in seconds)`),
            by = "name")


if(task_mapping_irr_survey_id == surveys$id[2]){
  df.james.bayesian %>% write_csv('within-lab-4-task-irr.csv')
}else{
  df.james.bayesian %>% write_csv('mturk-4-task-irr.csv')
}
```

# Get Main IRR Analysis

This is the large KA block, that also doubles as taking care of drops and doing other IRR measures

```{r message=FALSE, warning=FALSE}
# Set up to look at a filtered version of the task map
df.task_map_from_filtered <-
  data.frame(matrix(NA, nrow = 0, ncol = length(unique(
    df.all_ratings_raw$task_name
  ))))
colnames(df.task_map_from_filtered) <-
  unique(df.all_ratings_raw$task_name)

kripp_alpha_per_question <- data.frame(
  question_name = c(),
  #num_tasks = c(),
  #num_raters = c(),
  cat_specific = c(),
  krippendorf_alpha = c(),
  mean_agreement = c(),
  mean_alpha = c()
  #class_balance = c()
  #kripp_matrix = c()
)

for (i in 1:length(main_questions)) {
  q_name <- main_questions[i]
  ### SET UP MATRIX OF RATERS AND RATINGS ####
  kripp_alpha_matrix <- df.all_ratings_raw %>%
    filter(question_name == q_name) %>%
    group_by(task_name) %>%
    left_join(df.task_map_summary %>% select(task_name, n_labels)) %>% # get n_labels
    pivot_wider(
      names_from = c(name),
      values_from = c(value),
      values_fn = list
    ) %>%
    ungroup() %>%
    select(-c(task_num, question_name, task_name, n_labels)) %>%
    mutate(across(everything(), as.character)) %>%
    mutate(across(everything(),
                  function(x)
                    case_when(
                      x == "Mental" ~ 0,
                      x == "Physical" ~ 1,
                      x == "No" ~ 0,
                      x == "Yes" ~ 1,
                      #x == "Not applicable or not answerable based on the task description (Please Elaborate Below.)" ~ 3,!is.na(as.numeric(x)) ~ as.numeric(x)
                    ))) %>%
    as.matrix() %>%
    t()
  
  ##### IGNORE THE NUMERIC MEASURES --- NOT USEFUL FOR NOW ####
  kripp_alpha <- NA
  # these are numeric
  if (q_name == 'Q2intel_manip_1' |
      q_name == 'Q21intellective_judg_1' |
      q_name == 'Q5creativity_input_1') {
    # kripp_alpha <- kripp_alpha_matrix %>% kripp.alpha(method = "interval")
    invisible()
    
  } else{
    ### GET HOW MUCH PEOPLE DISAGREE - SET UP ###
    # figure out agreement level of people
     # setting up for dropping raters who disagree a lot
    # keep track of how many deviations each person has
    num_deviations <- rep(0, length(rownames(kripp_alpha_matrix)))
    modes <- apply(kripp_alpha_matrix, 2, function(x) {Mode(x , na.rm = T)})
    # handle ties
    lengths(modes)==1
    modes <- unlist(ifelse(lengths(modes)==1, modes, NA))
    
    for (person_i in 1:nrow(kripp_alpha_matrix)){
      person_i_name <- rownames(kripp_alpha_matrix)[person_i]
      person_resp <- kripp_alpha_matrix[person_i,]
      person_i_sum <- sum(xor(modes,person_resp), na.rm = T)
      num_deviations[person_i] <- person_i_sum
    }

    ### DROP THE N WORST PEOPLE -- PER QUESTION
    # N_Worst <- 2 # set the number of "worst" that we want to drop
    # worst_indices <- tail(order(num_deviations), N_Worst)
    # kripp_alpha_matrix <- slice(as.data.frame(kripp_alpha_matrix),-c(worst_indices)) %>% as.matrix()
    
    #### CALCULATE THE IMPORTANT STUFF
    kripp_alpha <- kripp_alpha_matrix %>% kripp.alpha(method = "nominal")
  
      class_balance_1 <-
    sum(kripp_alpha_matrix, na.rm = T) / (sum(kripp_alpha_matrix == 0, na.rm =T) + sum(kripp_alpha_matrix, na.rm = T))
      class_balance_0 <-
    sum(kripp_alpha_matrix == 0, na.rm = T) / (sum(kripp_alpha_matrix == 0, na.rm = T) + sum(kripp_alpha_matrix, na.rm = T))
    
      
   ### CALCULATE AGREEMENT ###
    agreements <- c()
    alphas <- c()
    for(task_i in 1:ncol(kripp_alpha_matrix)){
      mean_rating = mean(kripp_alpha_matrix[,task_i], na.rm = T)
      n_labels = length(kripp_alpha_matrix[,task_i])
      agreement = ifelse(mean_rating > 0.5, mean_rating, 1-mean_rating)
      general.alpha = (agreement-0.5)/0.5
      alphas[task_i] <- general.alpha
      agreements[task_i] <- agreement
    }
    
  df.curQmatrix <- data.frame(t(colMeans(kripp_alpha_matrix, na.rm = T)))
  colnames(df.curQmatrix) <- unique(df.all_ratings_raw$task_name)
  
  df.task_map_from_filtered <- rbind(df.task_map_from_filtered, 
                                     df.curQmatrix)
    
  #### RETURN THE IMPORTANT STUFF #####
  kripp_alpha_per_question <- kripp_alpha_per_question %>% rbind(
    data.frame(
      question_name = q_name,
      #num_tasks = kripp_alpha$subjects,
      num_raters = kripp_alpha$raters,
      #cat_specific = cat_specific,
      krippendorf_alpha = kripp_alpha$value,
      class_balance = min(class_balance_1, class_balance_0, na.rm = T),
      mean_agreement = mean(agreements, na.rm = T),
      mean_alpha = mean(alphas, na.rm = T)
      #kripp_matrix = kripp_alpha_matrix
    )
  )
    
  }
}
```

```{r}
kripp_alpha_per_question %>% arrange(desc(mean_alpha))
```


Set up the version of the task map that is filtered (removing unreliable raters)

```{r}
df.task_map_from_filtered <- t(df.task_map_from_filtered) %>% data.frame()

names(df.task_map_from_filtered) <- c(
  "Q1concept_behav",
  "Q3type_1_planning",
  "Q4type_2_generate",
  "Q6type_5_cc",
  "Q7type_7_battle",
  "Q8type_8_performance",
  "Q9divisible_unitary",
  "Q10maximizing",
  "Q11optimizing",
  "Q13outcome_multip",
  "Q14sol_scheme_mul",
  "Q15dec_verifiability",
  "Q16shared_knowledge",
  "Q17within_sys_sol",
  "Q18ans_recog",
  "Q19time_solvability",
  "Q20type_3_type_4",
  "Q22confl_tradeoffs",
  "Q23ss_out_uncert",
  "Q24eureka_question"
)
  
df.task_map_from_filtered <- df.task_map_from_filtered %>% rownames_to_column("task_name")
```

# Version of map done with the filtered data (1 unreliable rater dropped)
```{r}
set.seed(1)
pca.filtered <- df.task_map_from_filtered %>%
  select(-task_name) %>%
  prcomp()

p<-cbind(df.task_map_from_filtered,
      pca.filtered$x) %>%
  ggplot(
    aes(x = PC1,
    y = PC2,
    label = task_name)
  ) + geom_point() + geom_label(nudge_y = 0.05, size = 2) 

ggsave(plot = p, filename = 'task-map-filtered-irr.png')

combined_data <- cbind(df.task_map_from_filtered,
      pca.filtered$x)

plot_ly(
  x = combined_data$PC1,
  y = combined_data$PC2,
  z = combined_data$PC3,
  type = "scatter3d",
  mode = "markers", # can use mode = "text"
  text = combined_data$task_name
)
```

Look at qualitative feedback about questions
```{r}
df.all_ratings_raw %>%
  filter(question_name %in% feedback_questions) %>%
  arrange(question_name) #%>% View()
```



