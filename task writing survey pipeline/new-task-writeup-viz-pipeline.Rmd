---
title: "new-task-writeup-viz-pipeline"
author: "Emily Hu"
date: "2/27/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(stats)
library(corrplot)
library(irr)
library(icr)
library(stringr)
library(markdown)
library(qualtRics)
library(ggplot2)
library(dplyr)
library(tidyverse)
```

# Basic setup
(Not pushed to GitHub)

# Pull in Survey Data
```{r}
# Qualtrics API Access

qualtrics_api_credentials(api_key = "hs4TCG4lZ0PzozF9ICTtfSApwx1LPeFHg2e5RwUo", 
                          base_url = "upenn.co1.qualtrics.com",
                          overwrite = TRUE)

surveys <- all_surveys()

task_writing_survey_id <- surveys$id[2]

df.survey_results <- fetch_survey(
  surveyID = task_writing_survey_id,
  save_dir = ".",
  force_request = T
)
```

# Check timing
```{r}
# df.survey_results %>%
#   mutate(
#     Duration = EndDate-StartDate
#   ) %>%
#   summarize(
#     medianDuration = median(Duration)
#   )
```

```{r}
colnames(df.survey_results)

df.survey_results_cleaned <-df.survey_results %>%
  select(
    `task-name`,
    `GitHub-id`,
    `stimulus-complex`,
    `goal-directives`,
    skills,
    `ui-ux`,
    `allowed-gr-proc`,
    `GitHub-link`,
    `external-resources`
  )

df.survey_results_cleaned
```

# Generate .md Files
```{r}
for(i in 1:nrow(df.survey_results_cleaned)){
  row <- df.survey_results_cleaned[i,]
  task_title <- paste0("# ",toString(row$`task-name`),"\n\n")
  author <- paste0("Mapped by: ",toString(row$`GitHub-id`)," \n\n")
  stim_complex <- paste0("## 1. Stimulus Complex \n",toString(row$`stimulus-complex`),"\n\n")
  goal_directives <- paste0("## 2. Goal Directives \n",toString(row$`goal-directives`),"\n\n")
  allowed_group_processes <- "## 3. Allowed Group Processes \n" 
  group_proc_info <- "The following details are not the main parts of the task, but rather additional information about ways in which participants could interact.\n\n"
  skills <- paste0("### Skills \n",toString(row$`skills`),"\n\n")
  uiux <- paste0("### UI-UX Allowed Processes\n",toString(row$`ui-ux`),"\n\n")
  other_allowed_proc <- paste0("### Other Allowed Processes\n",toString(row$`allowed-gr-proc`),"\n\n")
  github <- paste0("## GitHub Link \n",toString(row$`GitHub-link`))

  total_markdown_string <- paste0(task_title,
                                  author,
                                  stim_complex,
                                  goal_directives,
                                  allowed_group_processes,
                                  group_proc_info,
                                  skills,
                                  uiux,
                                  other_allowed_proc,
                                  github)
  
  file_title <- paste0("../tasks/summaries/",row$`task-name`,"-writeup.md")
  
  # only write the file if it doesn't already exist
  if(file.exists(file_title) == FALSE){
    writeup_file <- file(file_title, "w") # Create Rmarkdown file
    cat(total_markdown_string, file = writeup_file) # Write your content to Rmarkdown file
    close(writeup_file)
  }
}
```

# Generate a CSV with task names in one column and HTML in a second column
This will enable us to paste directly into the task mapping survey.
```{r}
df.task_map_format <- data.frame(matrix(ncol = 2, nrow = 0))
colnames(df.task_map_format) <- c('Task Name', 'Task Writeup')

for(i in 1:length(df.survey_results$`task-name`)){
  task_name <- df.survey_results$`task-name`[i]
  file_title <- paste0("../tasks/summaries/",task_name,"-writeup.md")
  
  txt <- read_file(file_title)
  txt_html <- markdownToHTML(text = txt,
                             fragment.only =  T)
  
  # remove the mapper name and the github link
  txt_html <- str_remove(txt_html, "<p>Mapped by: .*</p>") %>%
      str_remove("<h2>.*GitHub Link.*") %>%
      str_remove("<a href=.*</a>") %>%
      str_remove("<p></p>")
  
  # remove line breaks
  txt_html <- str_replace_all(txt_html, "[\r\n]" , "")
  
  # add better line breaks
  txt_html <- str_replace_all(txt_html, "<h2>", "<br><h2>") %>%
              str_replace_all("<h1>", "<h2>") %>%
              str_replace_all("</h1>", "</h2>")
  
  new_row = data.frame(
    'Task Name'= task_name,
    'Task Writeup' =  txt_html
  )
  
  df.task_map_format <- rbind(df.task_map_format, new_row)
}
```

Write mapped tasks into HTML for rating
```{r}
df.task_map_format %>% write_csv('./html/task-htmls.csv')
```


# Look at Survey Results for Task MAPPING

Get a mapping of the task name to the loop numbers
```{r}
df.task_name_to_index <- cbind(df.survey_results$`task-name`, rep(1:length(df.survey_results$`task-name`))) %>%
                                as.data.frame() %>%
                                rename(
                                  task_name = V1,
                                  task_num = V2
                                ) %>%
                                mutate(
                                  task_num = strtoi(task_num)
                                )
```

Get survey data
```{r}
task_mapping_survey_id <- surveys$id[3]

df.task_map <- fetch_survey(
  surveyID = task_mapping_survey_id,
  save_dir = ".",
  force_request = TRUE
)

df.task_map
```

Names of the questions
```{r}
# Question names
main_questions <- c(
  "Q1concept_behav",
  "Q2intel_manip_1",
  "Q3type_1_planning",
  "Q4type_2_generate",
  "Q5creativity_input_1",
  "Q6type_5_cc",
  "Q7type_7_battle",
  "Q8type_8_performance",
  "Q9divisible_unitary",
  "Q10maximizing",
  "Q11optimizing",
  "Q13outcome_multip",
  "Q14sol_scheme_mul",
  "Q15dec_verifiability",
  "Q16shared_knowledge",
  "Q17within_sys_sol",
  "Q18sol_eureka",
  "Q19time_solvability",
  "Q20type_3_type_4",
  "Q21intellective_judg_1",
  "Q22confl_tradeoffs",
  "Q23ss_out_uncert"
)

feedback_questions <- c(
  "Q1concept_beh_feedbk",
  "Q2intel_manip_fdbk",
  "Q3type_1_feedback",
  "Q4type_2_feedbck",
  "Q5creativity_feedbck",
  "Q6type_5_feedback",
  "Q7type_7_feedback",
  "Q8type_8_feedback",
  "Q9div_unitary_feedbk",
  "Q10maximizing_feedbk",
  "Q11optimizing_feedbk",
  "Q13out_mult_feedbk",
  "Q14sol_s_mult_fdbk",
  "Q15d_verif_feedbk",
  "Q16shared_kno_feedbk",
  "Q17within_sys_feedbk",
  "Q18sol_eureka_feedbk",
  "Q19time_solv_feedbk",
  "Q20type_3_4_feedbk",
  "Q21int_jud_feedbk",
  "Q22con_trade_feedbk",
  "Q23ss_outc_u_feedbk"
)

confidence_questions <- c(
  "Q1concept_beh_conf",
  "Q2intel_manip_conf",
  "Q3type_1_conf",
  "Q4type_2_conf",
  "Q5creativity_conf",
  "Q6_type_5_conf",
  "Q7type_7_conf",
  "Q8_type_8_conf",
  "Q9div_unitary_conf",
  "Q10maximizing_conf",
  "Q11optimizing_conf",
  "Q13out_mult_conf",
  "Q14sol_s_mult_conf",
  "Q15d_verif_conf",
  "Q16shared_kno_conf",
  "Q17within_sys_conf",
  "Q18sol_eureka_conf",
  "Q19time_solv_conf",
  "Q20type_3_4_conf",
  "Q21int_jud_conf",
  "Q22con_trade_conf",
  "Q23ss_out_u_conf"
)

# For getting median time
df.task_map%>%
  mutate(
    Duration = EndDate-StartDate
  ) %>%
  summarize(
    medianDuration = median(Duration),
    sdDuration = sd(Duration)/60
  )
```

Gather all of the raw ratings
```{r}
df.task_map <- df.task_map %>% # need to only filter for complete responses
  filter(Finished == T)

df.all_ratings_raw <- df.task_map %>%
  select(c(matches('[0-9]_'))) %>%
  mutate(across(everything(), as.character)) %>%
  cbind(df.task_map %>%  # Get the identities of the raters; for now - use IPAddress, but eventually will use `name`
          # Need to cleave off the last two parts of the IPAddress bc they vary too much by person
          mutate(IPAddress = as.factor(IPAddress))%>% select(IPAddress)) %>%
          #mutate(IPAddress = as.factor(str_extract(IPAddress, "[0-9]*\\.[0-9]*\\."))) %>% select(IPAddress)) %>%
  group_by(IPAddress) %>% # Group by the rater
  pivot_longer(
    cols = -c(IPAddress),
    names_sep = 2,
    names_to = c("task_num", "question_name")
  ) %>%
  mutate(task_num = strtoi(gsub("[^0-9]", "", task_num)),
         question_name = str_extract(question_name, "Q[0-9].*")) %>% 
  drop_na() %>%
  group_by(task_num) %>%
  inner_join(df.task_name_to_index,
            by = "task_num")
```

# Get number of ratings so far
```{r}
df.all_ratings_raw %>%
  filter(question_name == "Q1concept_behav") %>%
  ungroup() %>%
  select(task_name) %>%
  table()
```

```{r}
df.all_ratings_raw %>%
  filter(value == "Not applicable or not answerable based on the task description (Please Elaborate Below.)") %>%
  mutate(value = 1) %>%
  pivot_wider(names_from = "question_name", values_from = value) %>%
  group_by(task_name) %>%
  summarize(
    across(matches('Q'), ~sum(., na.rm = T))
  ) %>%
  View()


  # mutate(
  #   row_sum = rowSums(across(matches('Q'))),
  #   col_sum = colSums(across(matches('Q')))
  # ) %>% 
```

Looking at inter-rater reliability

1. General Agreement and Alpha Per Question, Per Task
```{r}
# This is the expected level of disagreement based on the number of datapoints
get_expected_agreement <- function(r){
  expected_prob <- round(0.5*r)/r
  return(ifelse(expected_prob > 0.5, expected_prob, 1-expected_prob))
}

# main summary stats
df.main_questions_summary <- df.all_ratings_raw %>%
  filter(question_name %in% main_questions) %>%
  filter(question_name != 'Q2intel_manip_1' & 
            question_name != 'Q21intellective_judg_1' &
            question_name != 'Q5creativity_input_1') %>%
  mutate(value = recode(value, 
                     "Mental" = 0,
                     "Physical" = 1,
                     "Not applicable or not answerable based on the task description (Please Elaborate Below.)" = 3,
                     "No" = 0,
                     "Yes" = 1
                   )) %>%
  filter(value != 3) %>% # remove cases in which people thought it wasn't answerable - for now
  group_by(task_name,question_name) %>%
  summarize(mean_rating = mean(value),
            n_labels = agree(t(value))$raters,
            all_values = paste(value, collapse = " & "),
            # TODO - this is a manual way of calculating percent agreement
            agreement = ifelse(mean_rating > 0.5, mean_rating, 1-mean_rating),
            # TODO - this is a manual way of calculating Kripp's Alpha
            exp.prob = get_expected_agreement(n_labels),
            general.alpha = 1-((1-agreement)/(1-get_expected_agreement(n_labels)))
            )

df.task_map_summary <- df.main_questions_summary %>%
  filter(n_labels > 1) %>%
  group_by(task_name) %>%
  summarize(
    n_labels = round(mean(n_labels),0),
    mean_agreement = mean(agreement),
    mean_alpha = mean(general.alpha)
  )

df.task_map_summary
```
Get level of agreement per task and per question
```{r}
df.per_task_q_agreement <-
  df.main_questions_summary %>% filter(n_labels > 1) %>% 
  select(task_name, question_name, agreement) %>%
  pivot_wider(names_from = question_name, values_from = agreement) %>%
  drop_na()

# sort by columnns
df.per_task_q_agreement <- df.per_task_q_agreement[,-1][,order(colSums(df.per_task_q_agreement[,-1],na.rm=T))] %>%
  cbind(df.per_task_q_agreement$task_name) %>%
  rename(
    task_name = `df.per_task_q_agreement$task_name`
  ) %>%
  relocate(task_name)

# sort by rows
df.per_task_q_agreement <- df.per_task_q_agreement %>%
  mutate(
    rowsum = rowSums(across(where(is.numeric)))
  ) %>%
  arrange(rowsum) %>%
  select(-rowsum)
  
df.per_task_q_agreement %>%
  write_csv('per-task-per-q-agreement.csv')
```

2. Krippendorf's Alpha - Across the 22 questions, aggregating all questions
```{r message=FALSE, warning=FALSE}
kripp_alpha_per_question <- data.frame(
  question_name = c(),
  num_tasks = c(),
  num_raters = c(),
  krippendorf_alpha = c()
)

for(i in 1:length(main_questions)){
  q_name <- main_questions[i]
  
  kripp_alpha_matrix <- df.all_ratings_raw %>%
  filter(question_name == q_name) %>%
  group_by(task_name) %>%
  left_join(df.task_map_summary %>% select(task_name, n_labels)) %>% # get n_labels
  filter(n_labels > 3) %>% # remove tasks with just 2 raters
  pivot_wider(names_from = c(IPAddress),
              values_from = c(value),
              values_fn = list) %>%
  mutate(across(matches('[0-9]'), as.character)) %>%
  mutate(across(matches('[0-9]'),
                function(x) case_when( # 3's are treated as NA, so not recoded
                  x == "Mental" ~ 0,
                  x == "Physical" ~ 1,
                  x == "No" ~ 0,
                  x == "Yes" ~ 1,
                  x == "Not applicable or not answerable based on the task description (Please Elaborate Below.)" ~ 3,
                  x == "c(\"Yes\", \"Yes\")" ~ 1,
                  x == "c(\"No\", \"No\")" ~ 0,
                  !is.na(as.numeric(x)) ~ as.numeric(x)
                ))) %>%
    ungroup() %>%
    select(matches('[0-9]')) %>%
    as.matrix() %>%
    t()
  
    kripp_alpha <- NA
    # these are numeric
    if(q_name == 'Q2intel_manip_1' | 
            q_name == 'Q21intellective_judg_1' |
            q_name == 'Q5creativity_input_1'){
      kripp_alpha <- kripp_alpha_matrix %>% krippalpha(metric = "interval")
    }else{
      kripp_alpha <- kripp_alpha_matrix %>% krippalpha(metric = "nominal")
    }
    
    kripp_alpha_per_question <- kripp_alpha_per_question %>% rbind(
      data.frame(
        question_name = q_name,
        num_tasks = kripp_alpha$n_units,
        num_raters = kripp_alpha$n_coders,
        krippendorf_alpha = kripp_alpha$alpha
        # kripp_matrix = kripp_alpha_matrix
    ))
}

kripp_alpha_per_question %>% arrange(desc(krippendorf_alpha))
```

```{r}
kripp_alpha_per_question %>% arrange(desc(krippendorf_alpha)) %>% write_csv("krippendorf_alpha_per_question.csv")
```



Look at confidence

```{r, fig.width= 18}
p <- df.all_ratings_raw %>%
  filter(question_name %in% confidence_questions) %>%
  mutate(
    value = recode(value,
                   "Not at all confident" = "1",
                   "Not confident" = "2",
                   "Neutral" = "3",
                   "Confident" = "4",
                   "Very confident" ="5"
                   )
  ) %>%
  ggplot(
    aes(
      x = value,
      fill = task_name
    )
  ) +
  geom_bar() 

p + facet_wrap(~question_name, scales = "free")

ggsave('./question-confidence-analysis.png')
```


Compare to the previous task map
```{r warning=FALSE}
df.task_map_new <- df.all_ratings_raw %>% mutate(value = recode(value, 
                     "Mental" = 0,
                     "Physical" = 1,
                     "Not applicable or not answerable based on the task description (Please Elaborate Below.)" = 3,
                     "No" = 0,
                     "Yes" = 1
                   )) %>%
    filter(question_name %in% main_questions) %>%
    filter(value != 3 && !is.na(value)) %>% # remove NA's, focus only on the task map features
    pivot_wider(names_from = "question_name") %>% # Note that there are 2 repeated labels!
    group_by(task_name) %>%
    select(-c(IPAddress, task_num)) %>%
    summarize( # note that repeated labels are just being averaged in
      Q1concept_behav = mean(unlist(Q1concept_behav)),
      Q3type_1_planning = mean(unlist(Q3type_1_planning)),
      Q4type_2_generate = mean(unlist(Q4type_2_generate)),
      Q6type_5_cc = mean(unlist(Q6type_5_cc)),
      Q7type_7_battle = mean(unlist(Q7type_7_battle)),
      Q8type_8_performance = mean(unlist(Q8type_8_performance)),
      Q9divisible_unitary = mean(unlist(Q9divisible_unitary)),
      Q10maximizing = mean(unlist(Q10maximizing)),
      Q11optimizing = mean(unlist(Q11optimizing)),
      Q13outcome_multip = mean(unlist(Q13outcome_multip)),
      Q14sol_scheme_mul = mean(unlist(Q14sol_scheme_mul)),
      Q15dec_verifiability = mean(unlist(Q15dec_verifiability)),
      Q16shared_knowledge = mean(unlist(Q16shared_knowledge)),
      Q17within_sys_sol = mean(unlist(Q17within_sys_sol)),
      Q18sol_eureka = mean(unlist(Q18sol_eureka)),
      Q19time_solvability = mean(unlist(Q19time_solvability)),
      Q20type_3_type_4 = mean(unlist(Q20type_3_type_4)),
      Q22confl_tradeoffs = mean(unlist(Q22confl_tradeoffs)),
      Q23ss_out_uncert = mean(unlist(Q23ss_out_uncert))
    )
  
# get old task map
df.task_map_old_names <- read_csv('../task_map.csv')
df.task_map_old <- cbind(df.task_map_old_names[,1],read_csv('../task_map_numeric.csv'))
```
```{r}
# get them to the same dimensions :)
df.task_map_old <- df.task_map_old %>%
  filter(task %in% df.task_map_new$task_name) %>% arrange(task)

df.task_map_new <- df.task_map_new %>%
  filter(task_name %in% df.task_map_old$task) %>% arrange(task_name)

# START WITH CORRELATIONS

# Q3type_1_planning
cor(df.task_map_new$Q3type_1_planning, df.task_map_old$generate_plan)
cor(df.task_map_new$Q3type_1_planning, df.task_map_old$generate_process)

# Q4type_2_generate
cor(df.task_map_new$Q4type_2_generate, df.task_map_old$generate_ideas)
cor(df.task_map_new$Q4type_2_generate, df.task_map_old$generate_creativity)

# Q6type_5_cc
cor(df.task_map_new$Q6type_5_cc, df.task_map_old$conflict_cooperation)
cor(df.task_map_new$Q6type_5_cc, df.task_map_old$conflict_interests)

# Q7type_7_battle
cor(df.task_map_new$Q7type_7_battle, df.task_map_old$external_adversary)

# Q8type_8_performance
cor(df.task_map_new$Q8type_8_performance, df.task_map_old$external_standard)
cor(df.task_map_new$Q8type_8_performance, df.task_map_old$goal_standard)

# Q9divisible_unitary
cor(df.task_map_new$Q9divisible_unitary, df.task_map_old$task_divisibility)
cor(df.task_map_new$Q9divisible_unitary, df.task_map_old$skills_specialization)

# Q10maximizing
cor(df.task_map_new$Q10maximizing, df.task_map_old$goal_maximize)

# Q11optimizing
cor(df.task_map_new$Q11optimizing, df.task_map_old$goal_optimize)

# Q13outcome_multip
cor(df.task_map_new$Q13outcome_multip, df.task_map_old$solution_multiplicity)
cor(df.task_map_new$Q13outcome_multip, df.task_map_old$solution_singularity)
cor(df.task_map_new$Q13outcome_multip, df.task_map_old$objective_multiplicity)

#Q14sol_scheme_mul
cor(df.task_map_new$Q14sol_scheme_mul, df.task_map_old$solution_path)
cor(df.task_map_new$Q14sol_scheme_mul, df.task_map_old$objective_multiplicity)

#Q15dec_verifiability
cor(df.task_map_new$Q15dec_verifiability, df.task_map_old$solution_verifiability)

#Q16shared_knowledge
cor(df.task_map_new$Q15dec_verifiability, df.task_map_old$solution_demonstrability)

#Q17within_sys_sol
cor(df.task_map_new$Q17within_sys_sol, df.task_map_old$solution_demonstrability)
cor(df.task_map_new$Q17within_sys_sol, df.task_map_old$solution_valid)

#Q18sol_eureka
cor(df.task_map_new$Q18sol_eureka, df.task_map_old$solution_demonstrability)

#Q19time_solvability
cor(df.task_map_new$Q19time_solvability, df.task_map_old$individual_theoretical)
cor(df.task_map_new$Q19time_solvability, df.task_map_old$individual_practical)

#Q20type_3_type_4
cor(df.task_map_new$Q20type_3_type_4, df.task_map_old$solution_objectivity)
cor(df.task_map_new$Q20type_3_type_4, df.task_map_old$solution_verifiability)

#Q22confl_tradeoffs
cor(df.task_map_new$Q22confl_tradeoffs, df.task_map_old$resolve_information)
cor(df.task_map_new$Q22confl_tradeoffs, df.task_map_old$resolve_data)
cor(df.task_map_new$Q22confl_tradeoffs, df.task_map_old$metric_multiplicity_tradeoff)
cor(df.task_map_new$Q22confl_tradeoffs, df.task_map_old$stakeholder_multiplicity)

#Q23ss_out_uncert
cor(df.task_map_new$Q23ss_out_uncert, df.task_map_old$measure_easy)
cor(df.task_map_new$Q23ss_out_uncert, df.task_map_old$measure_immediate)

# physical/mental
cor(df.task_map_new$Q1concept_behav, df.task_map_old$effort_mental)
```

```{r}
taskmap_new_mat <- as.matrix(df.task_map_new %>% select(-c(task_name)))
taskmap_old_mat <- as.matrix(df.task_map_old %>% select(generate_plan, generate_ideas, conflict_interests, external_adversary, external_standard, goal_standard, task_divisibility, skills_specialization, goal_maximize, goal_optimize, solution_singularity, solution_path, solution_verifiability, solution_demonstrability, solution_objectivity, resolve_information, resolve_data, metric_multiplicity_tradeoff, stakeholder_multiplicity))
taskmap_new_mat <- taskmap_new_mat[, qr(taskmap_new_mat)$pivot[seq_len(qr(taskmap_new_mat)$rank)]]
taskmap_old_mat <- taskmap_old_mat[, qr(taskmap_old_mat)$pivot[seq_len(qr(taskmap_old_mat)$rank)]]

lm(taskmap_new_mat~taskmap_old_mat) %>% summary()
```


